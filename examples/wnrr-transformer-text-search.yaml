# wnrr-conve-1vsAll-kl
job.type: search
search.type: ax
dataset.name: wnrr

# training settings (fixed)
train:
  max_epochs: 400
  auto_correct: True

# this is faster for smaller datasets, but does not work for some models (e.g.,
# TransE due to a pytorch issue) or for larger datasets. Change to spo in such
# cases (either here or in ax section of model config), results will not be
# affected.
negative_sampling.implementation: sp_po
negative_sampling:
  shared: True
  with_replacement: False
  implementation: batch
  num_samples:
    s: 512

# validation/evaluation settings (fixed)
valid:
  every: 5
  metric: mean_reciprocal_rank_filtered_with_test
  filter_with_test: True
  early_stopping:
    patience: 10
    min_threshold.epochs: 50
    min_threshold.metric_value: 0.15

eval:
  batch_size: 100000
  metrics_per.relation_type: True

entity_ranking:
  chunk_size: 128

# settings for reciprocal relations (if used)
import: [transformer, reciprocal_relations_model]
reciprocal_relations_model.base_model.type: transformer

# ax settings: hyperparameter serach space
ax_search:
  num_trials: 30
  num_sobol_trials: 30 # differs in the files
  parameters:
      # model
    - name: model
      type: fixed
      value: reciprocal_relations_model

    # training hyperparameters
    - name: train.batch_size
      type: fixed
      #values: [128, 256, 512, 1024] #1024 with 512 subbatches faster than 512 batch_size, so it is feasible
      #is_ordered: True
      value: 512
    - name: train.type
      type: fixed
      value: negative_sampling
    - name: train.optimizer.default.type
      type: fixed
      #values: [Adam, Adagrad] # Adamax and Adam are roughly equal in memory usage, Adagrad less
      value: Adagrad
    - name: train.loss
      type: fixed
      value: kl
    - name: train.loss_arg # label_smoothing implemented by S Chen, only on link prediction, not on mlm
      type: fixed
      # bounds: [-0.4, 0.2]
      value: 0.0
    - name: train.optimizer.default.args.lr
      type: fixed
      value: 0.01
      log_scale: True
    - name: train.lr_scheduler
      type: fixed
      value: ReduceLROnPlateau
    - name: train.lr_scheduler_args.factor
      type: fixed
      # bounds: [0.5, 0.95]
      value: 0.7
    - name: train.lr_scheduler_args.patience
      type: fixed
      value: 2
    - name: train.lr_scheduler_args.threshold
      type: fixed
      value: 0.0001
    - name: train.lr_warmup
      type: fixed
      value: 50

    # embedding dimension
    - name: lookup_embedder.dim
      type: fixed
      #values: [128, 256, 320] # 512 only with subbatches and more than double the time of 256 (1017s per epoch to 418s), include 320 as the original proposed?
      #is_ordered: True
      value: 320

    # embedding initialization
    - name: lookup_embedder.initialize
      type: choice
      values: [xavier_normal_, xavier_uniform_, normal_, uniform_]
    - name: lookup_embedder.initialize_args.normal_.mean
      type: fixed
      value: 0.0
    - name: lookup_embedder.initialize_args.normal_.std
      type: range
      bounds: [0.00001, 1.0]
      log_scale: True
    - name: lookup_embedder.initialize_args.uniform_.a
      type: range
      bounds: [-1.0, -0.00001]
    - name: lookup_embedder.initialize_args.xavier_uniform_.gain
      type: fixed
      value: 1.0
    - name: lookup_embedder.initialize_args.xavier_normal_.gain
      type: fixed
      value: 1.0

    # embedding regularization
    - name: lookup_embedder.regularize
      type: choice
      values: ['', 'l3', 'l2', 'l1']
      is_ordered: True
    - name: lookup_embedder.regularize_args.weighted
      type: choice
      values: [True, False]
    - name: reciprocal_relations_model.base_model.entity_embedder.regularize_weight
      type: range
      bounds: [1.0e-20, 1.0e-01]
      log_scale: True
    - name: reciprocal_relations_model.base_model.relation_embedder.regularize_weight
      type: range
      bounds: [1.0e-20, 1.0e-01]
      log_scale: True

    # embedding dropout
    # using using output_dropout instead as the model does the dropout
    # another time between the transformers
    # why negative?
    # 0.8
    # embedding dropout
    - name: reciprocal_relations_model.base_model.entity_embedder.dropout
      type: range
      bounds: [0.0, 0.8]
    - name: reciprocal_relations_model.base_model.relation_embedder.dropout
      type: range
      bounds: [0.0, 0.8]
    - name: reciprocal_relations_model.base_model.text_embedder.dropout
      type: range
      bounds: [0.0, 0.8]

    # training-type specific hyperparameters
    # model-specific entries
    - name: reciprocal_relations_model.base_model.encoder.num_layers
      type: fixed # change from original proposed?
      value: 3 # to make it comparable to 6 + 3 layers of hitter

    - name: reciprocal_relations_model.base_model.encoder.dropout
      type: range
      bounds: [0.0, 0.8]

    - name: reciprocal_relations_model.base_model.s_dropout
      type: range
      bounds: [0.0, 0.8]

    - name: reciprocal_relations_model.base_model.o_dropout
      type: range
      bounds: [0.0, 0.8]

    - name: reciprocal_relations_model.base_model.encoder.activation
      type: fixed
      value: gelu

    - name: reciprocal_relations_model.base_model.tokenization.trainer.min_frequency
      type: choice
      values: [1, 2, 3, 4]
      is_ordered: True
