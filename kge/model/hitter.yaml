import: [lookup_embedder]

# Should only be used with reciprocal relations:
# - model: reciprocal_relations_model
# - reciprocal_relations_model.base_model.type: transformer

hitter:
  class_name: Hitter
  entity_embedder:
    type: lookup_embedder
    +++: +++
  relation_embedder:
    type: lookup_embedder
    +++: +++

  # fraction of masked to recover, 0 -> no mlm
  mlm_fraction: 0.5

  implementation: pytorch
  # dropout for target embeddings before dot multiplication
  output_dropout: 0.0

  encoder:
    # see torch.nn.TransformerEncodeLayer
    nhead: 8                     # the number of heads
    dim_feedforward: 1280        # the dimension of the feedforward network model
    dropout: 0.1
    activation: gelu             # relu or gelu

    # see torch.nn.TransformerEncoder
    entity_encoder:
      num_layers: 3                # the number of sub-encoder-layers in the encoder
      dropout: 0.1

    context_encoder:
      num_layers: 6                # the number of sub-encoder-layers in the encoder
      dropout: 0.1


  # Initialization for transformer layers, CLS embedding and type embeddings.
  initialize: 'normal_'
  initialize_args:
    mean: 0.0
    std: 0.02
    +++: +++
