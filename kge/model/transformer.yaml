import: [lookup_embedder, text_lookup_embedder, shared_text_lookup_embedder]

# Should only be used with reciprocal relations:
# - model: reciprocal_relations_model
# - reciprocal_relations_model.base_model.type: transformer

transformer:
  class_name: Transformer
  entity_embedder:
    type: lookup_embedder
    +++: +++
  relation_embedder:
    type: lookup_embedder
    +++: +++
  text_embedder:
    type: text_lookup_embedder
    +++: +++
  relation_text_embedder:
    type: text_lookup_embedder
    +++: +++
  encoder:
    # see torch.nn.TransformerEncodeLayer
    nhead: 8                     # the number of heads
    dim_feedforward: 0        # the dimension of the feedforward network model
    dropout: 0.1
    activation: relu             # relu or gelu

    # see torch.nn.TransformerEncoder
    num_layers: 3                # the number of sub-encoder-layers in the encoder

  # Initialization for transformer layers, CLS embedding and type embeddings.
  initialize: 'normal_'
  initialize_args:
    mean: 0.0
    std: 0.02
    +++: +++

  embedding_dropout: 0.0

  enable_text: False
  enable_entity_structure: True
  enable_entity_text: False
  enable_relation_structure: True
  enable_relation_text: False
  entity_built_in_text_embedder: False
  relation_built_in_text_embedder: False
  text_source: entities
  tokenization:
    trainer:
      +++: +++

  s_dropout: 0.3
  o_dropout: 0.3
  p_dropout: 0.3
  s_dropout_loss: 1.0
  o_dropout_loss: 1.0
  p_dropout_loss: 1.0
  s_dropout_masked: 0.5
  o_dropout_masked: 0.5
  p_dropout_masked: 0.5
  s_dropout_replaced: 0.2
  o_dropout_replaced: 0.2
  p_dropout_replaced: 0.2
  s_text_dropout: 0.3
  o_text_dropout: 0.3
  p_text_dropout: 0.3
  s_text_dropout_loss: 1.0
  o_text_dropout_loss: 1.0
  p_text_dropout_loss: 1.0
  s_text_dropout_masked: 0.5
  o_text_dropout_masked: 0.5
  p_text_dropout_masked: 0.5
  s_text_dropout_replaced: 0.2
  o_text_dropout_replaced: 0.2
  p_text_dropout_replaced: 0.2

  mask_all_text: 0.0

  # equal, by_count
  self_pred_loss_weighing: by_count
